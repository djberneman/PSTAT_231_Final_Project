---
title: "Final Project"
author: "Dylan Berneman"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      toc_float: true
      toc_depth: '3'
      keep_md: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

# Introduction

The purpose of this project is to produce a model that will be able to predict the amount of people who contract COVID-19 in the United States as well as predict the amount of deaths that it causes.

## What is COVID-19?

Coronavirus disease 2019 (COVID-19) is a contagious disease caused by a virus, the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first known case was identified in Wuhan, China, in December 2019. The disease spread worldwide, leading to the COVID-19 pandemic.

Symptoms of COVIDâ€‘19 are variable and may imclude fever, cough, headache, fatigue, breathing difficulties, loss of smell, and loss of taste, pneumonia, dyspnea, hypoxia, respiratory failure, shock, multiorgan dysfunction, and death.

COVID-19 transmits when people breathe in air contaminated by droplets and small airborne particles containing the virus. On average, people are contagious two days before the first sign of symptoms and ten days after the first sign of symptoms. 

In mid-December of 2020, the first vaccines were made available to a limited amount of the public. The vaccines were released in a specific order to subsets of the populace. First starting with people over the age of 65 or with health conditions, then to people over the age of 18, before moving to people above the age of 12, and just recently to children age 5 and older.

## Why might this model be useful.

By comparing trends across individual states and the nation as a whole, it is possible that we could find commonalities between states with higher than average cases and deaths. In addition this could also help us identify which of the factors are most significant to the model and therefore, the pandemic. It could also reveal in what ways certain states fail in the implementation of vaccinations. Finally, it could reveal which age subset of the population has the most influence on the spread of the virus.

## Data and Packages

This project uses a combination of two data sets. Both data sets record the historical data of the states and territories of the United States. 

The first set of data is from the [nytimes/covid-19-data]("https://github.com/nytimes/covid-19-data"), which is an ongoing repository of data on coronavirus cases and deaths in the U.S. 

The second set of data is from the 
[COVID-19 Vaccinations in the United States, Jurisdiction](https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-Jurisdi/unsk-b7fc), which covers overall US COVID-19 vaccine deliveries and administration data at national and jurisdiction level.

Here are a few examples of the many variables contained within the combined data set:

  - `Date` : Date data are reported on CDC COVID Data Tracker
  - `Location` : Jurisdiction (State/Territory/Federal Entity)
  - `Cases` : The cumulative amount of cases recorded in the specified state
  - `Deaths` : The cumulative amount of deaths recorded in the specified state
  
Note: The complete codebook can be found in Appendix I
  
```{r include=FALSE}
# Load packages
library(corrplot)
library(corrr)
library(discrim)
library(ggplot2)
library(gridExtra)
library(ISLR)
library(ISLR2)
library(janitor)
library(klaR)
library(randomForest)
library(readr)
library(readxl)
library(rpart.plot)
library(tidymodels)
library(tidyverse)
library(tune)
library(vip)
library(xgboost)
```
```{r include=FALSE}
# read in data
Data <- read_xlsx("Final_Data.xlsx")
Data$Location <- as.factor(Data$Location)
```

# Data Cleaning and Modifications

Most of the data cleaning was done in excel due to the data sets being too large to properly manage and clean within RStudio. 

While neither data set had any major issues, they were arranged by their most recent observations. 

To make them more comprehensible, I sorted them the following ways:

  1.) Ascending Ordered the `Location` alphabetically
  2.) Ascending ordered the `Date` by least recent to most recent
  
I then removed any columns that I did not believe I would need. This included 1 column from the first set of data and 58 columns from the second set of data. 

I then created a column called `Days Since 1st Case` to denote the time that had passed since the first reported case of the coronavirus in the US.

In order to merge both data sets, I had to make sure that both started and ended on the same date for each location. For some locations, there are a month or two of more observations since they have the earliest cases. I then had to add the value of 0 to 43 of the columns for each observation that took place before the distribution of the vaccine. After making sure that the only values in `location` were the 50 states, 5 territories, D.C., and the United States as a whole, I finally merged both sets of data together.

After that, I decided to create 3 more new variables. To see the differences in deaths and cases between individual days, I created `New_Cases` and `New_Deaths`. While it was useful to have the cumulative historic data for `Cases` and `Deaths`, it only reveals the total amount of people who have contracted the virus or died from it. Additionally, because people are contagious an average of 12 days as stated above in "What is COVID-19?", I believed that it would be sensible to have a new variable, `Contagious`, be the sum of the current day's and previous 11 days' observations of `New_Cases`.

```{r}
# The first 1000 observations from the data set
Data
```
## Data Grouping

The observations for the United States are a sum of all other observations of the same date. For this reason I will be separating the data into two group. US will have the data for the United States while Jurisdictions will have the remaining data. This will be useful to determine if the prediction models of both groups are alike, seeing as they technically contain the exact same data. We will see if there are differences in the variables being adjusted for each location in Jurisdiction and for the nation as a whole in US.
```{r}
US <- subset(Data, Location == "United States")
US

States <- subset(Data, Location != "United States")
States
```

# Exploratory Data Analysis
```{r}
Current = subset(States, Days == 870)[c("Location", "Deaths")]
Current
Current <- arrange(Current, desc(Deaths))

head(Current, 15) %>% 
  ggplot(aes(Location, Deaths)) + geom_col() + coord_flip() + labs(title  = "Total Deaths by Location", x = "Location", y = "Deaths") + geom_text(aes(label = round(Deaths, 1)))
```
```{r}
Toll = subset(US, New_Cases >= 0)[c("Date", "New_Cases")]

ggplot(Toll, aes(Date, New_Cases)) + geom_line() + labs(title  = "New Cases Over Time", x = "Date", y = "New_cases")
```
```{r}
Fatalities = subset(US, New_Deaths >= 0)[c("Date", "New_Deaths")]

ggplot(Fatalities, aes(Date, New_Deaths)) + geom_line() + labs(title  = "Frequency of Lives Lost", x = "Date", y = "New_Deaths")
```
```{r}
Spread = subset(US, New_Cases >= 0)[c("Contagious", "New_Cases")]

ggplot(Spread, aes(Contagious, New_Cases), scale="exponential") + geom_point() + labs(title  = "Spread of COVID-19", x = "Contagious", y = "New_Cases")
```
```{r}
States$Location <- as.numeric(States$Location)
US %>%
  select(Days, Cases, New_Cases, Contagious, Deaths, New_Deaths, Distributed, Administered_Pct, Recip_Administered_Pct,	Administered_Dose1_Pop_Pct, Series_Complete_Pop_Pct, Additional_Doses_Vax_Pct,	POPESTIMATE2020) %>% 
  cor() %>% 
  corrplot(type = 'lower', diag = TRUE, method = 'number', tl.srt = 45, tl.offset = 0.5, tl.cex = 0.75, number.cex= 0.57) 

States %>%
  select(Days, Location, Cases, New_Cases, Contagious, Deaths, New_Deaths, Distributed, Administered_Pct, Recip_Administered_Pct,	Administered_Dose1_Pop_Pct, Series_Complete_Pop_Pct, Additional_Doses_Vax_Pct,	POPESTIMATE2020) %>% 
  cor() %>% 
  corrplot(type = 'lower', diag = TRUE, method = 'number', tl.srt = 45, tl.offset = 0.5, tl.cex = 0.75, number.cex= 0.57)
```
The results for the States correlation variables and for the US correlation variables reveals that there are major differences between what is revealed by each correlation plot. As can be seen within the lower right triangle in the US plot, almost all variables that are associated with vaccinations have high to extremely high correlations. When comparing those same correlation factors in the States plot, it is clear to see that the summation of all `Location` variables in the US data set skews the initial results of the State data set. This can be evidenced by the fact that there are 14 correlation factors in the US plot that are 0.99 or above. This would mean each variable of those correlation factors would increase and decrease at nearly the exact same rate. I find it very hard to believe that `Dist_Per_100K` and `Admin_Per_100K` would have the same rate of change because that would imply that almost no vaccines that are distributed around the country ever expire or go to waste. I also know for a fact that there were many administration centers that were offering the vaccine to people below the age of 65 at the time when those vaccines were reserved for them since they are considered the most vulnerable. The centers were being given more vaccines than they had people they were meant to administer them to. And the trouble with expired vaccines is that unlike food past its expiration date, vaccines past their expiration date may still leave people vulnerable to the virus.

In addition, the other part of the plots reveal that correlation factors involved with cases and deaths have stronger significance in the States data set. The correlation factors in the US variables make it look like the distribution and administration of the vaccine had very little effect on the amount of people who got infected and the amount who died. Since that would imply that taking a vaccine does not noticeably decreases your chancres of catching the virus, it would go against the scientific fact that vaccines are directly responsible for the prevention of viruses and diseases spreading.

```{r}
set <- subset(Data, Days == 870)

Set <- set %>%
   group_by(Location) %>%
   summarise(Pct_Death = Deaths / POPESTIMATE2020)

Set1 <- Set[order(-Set$Pct_Death),]

Set1a <- Set1[0:28,]
Set1b <- Set1[29:57,]

Set1a

plot1 <- ggplot(Set1a, aes(x = Pct_Death, y = Location)) + geom_col() + xlab("Population Death %") + ylab("Location")
plot1

plot2 <- ggplot(Set1b, aes(x = Pct_Death, y = Location)) + geom_col() + xlab("Population Death %") + ylab("Location")
plot2
```

# Model Building

The goal in this project will be to build a model from the States data set that most accurately predicts its own `Deaths` or `New_Deaths` and those of the US data set. Then, we will do the opposite and compare the results of both to see whether `States` or `US` more accurately predict both data sets.

## Data Splitting

```{r}
set.seed(91362)

Data_split <- initial_split(Data, prop = 0.7, strata = Location)
Data_train <- training(Data_split)
Data_test <- testing(Data_split)

US1_split <- initial_split(US, prop = 0.7, strata = Deaths)
US1_train <- training(US1_split)
US1_test <- testing(US1_split)

US2_split <- initial_split(US, prop = 0.7, strata = New_Deaths)
US2_train <- training(US2_split)
US2_test <- testing(US2_split)

States1_split <- initial_split(States, prop = 0.7, strata = Deaths)
States1_train <- training(States1_split)
States1_test <- testing(States1_split)

States2_split <- initial_split(States, prop = 0.7, strata = New_Deaths)
States2_train <- training(States2_split)
States2_test <- testing(States2_split)
```
```{r}
set.seed(345)

Data_folds <- vfold_cv(Data_train, strata = Location, v = 5)
US1_folds <- vfold_cv(US1_train, strata = Deaths, v = 5)
US2_folds <- vfold_cv(US2_train, strata = New_Deaths, v = 5)
States1_folds <- vfold_cv(States1_train, strata = Deaths, v = 5)
States2_folds <- vfold_cv(States2_train, strata = New_Deaths, v = 5)
```

```{r}
Data_train_recipe <- recipe(Location ~ Days + Cases + New_Cases + Deaths + New_Deaths + Contagious + Distributed + Administered + Administered_Pct +	Recip_Administered + Recip_Administered_Pct + Administered_Dose1_Recip + Administered_Dose1_Pop_Pct + Series_Complete_Yes + Series_Complete_Pop_Pct + Additional_Doses + Additional_Doses_Vax_Pct, data = Data_train)

US1_train_recipe <- recipe(Deaths ~ Days + Cases + New_Cases + Contagious +  Distributed + Administered + Administered_Pct +	Recip_Administered + Recip_Administered_Pct + Administered_Dose1_Recip + Administered_Dose1_Pop_Pct + Series_Complete_Yes + Series_Complete_Pop_Pct + Additional_Doses + Additional_Doses_Vax_Pct + POPESTIMATE2020, data = US1_train)

US2_train_recipe <- recipe(New_Deaths ~ Days + Cases + New_Cases + Contagious +  Distributed + Administered + Administered_Pct +	Recip_Administered + Recip_Administered_Pct + Administered_Dose1_Recip + Administered_Dose1_Pop_Pct + Series_Complete_Yes + Series_Complete_Pop_Pct + Additional_Doses + Additional_Doses_Vax_Pct + POPESTIMATE2020, data = US2_train)

States1_train_recipe <- recipe(Deaths ~ Location + Days + Cases + New_Cases + Contagious +  Distributed + Administered + Administered_Pct +	Recip_Administered + Recip_Administered_Pct + Administered_Dose1_Recip + Administered_Dose1_Pop_Pct + Series_Complete_Yes + Series_Complete_Pop_Pct + Additional_Doses + Additional_Doses_Vax_Pct + POPESTIMATE2020, data = States1_train)

States2_train_recipe <- recipe(New_Deaths ~ Location + Days + Cases + New_Cases + Contagious +  Distributed + Administered + Administered_Pct +	Recip_Administered + Recip_Administered_Pct + Administered_Dose1_Recip + Administered_Dose1_Pop_Pct + Series_Complete_Yes + Series_Complete_Pop_Pct + Additional_Doses + Additional_Doses_Vax_Pct + POPESTIMATE2020, data = States2_train)
```
```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(Data_train_recipe)

log_fit <- fit(log_wkflow, Data_train)
log_fit

lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(Data_train_recipe)

lda_fit <- fit(lda_wkflow, Data_train)
lda_fit
```
```{r}
log_acc <- predict(log_fit, new_data = Data_train, type = "class") %>% 
  bind_cols(Data_train %>% select(Location)) %>% 
  accuracy(truth = Location, estimate = .pred_class)
lda_acc <- predict(lda_fit, new_data = Data_train, type = "class") %>% 
  bind_cols(Data_train %>% select(Location)) %>% 
  accuracy(truth = Location, estimate = .pred_class)

results <- bind_rows(log_acc, lda_acc) %>% 
  tibble() %>% mutate(model = c("Logistic", "LDA")) %>% 
  select(model, .estimate) %>% 
  arrange(.estimate)

results
```
```{r}
lda_test <- fit(lda_wkflow, Data_test)
predict(lda_test, new_data = Data_test, type = "class") %>% 
  bind_cols(Data_test %>% select(Location)) %>% 
  accuracy(truth = Location, estimate = .pred_class)
```
Considering that there are 47,025 observations in the data set as well as 57 possible locations that are being predicted, an accuracy rate of nearly 29% is not too terrible. Nonetheless, the model.
```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(Data_train_recipe)

param_grid <-grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = Data_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc),
  control = control_grid(verbose = TRUE)
  )

tune.res <- autoplot(tune_res)
tune.res
```
```{r}
complexity <- arrange(collect_metrics(tune_res), desc(mean))
best_complexity <- complexity[1,]
best_complexity

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = Data_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

